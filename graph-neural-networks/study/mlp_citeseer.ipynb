{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a24091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmocak\\PycharmProjects\\machine-learning-circus\\.venv\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7c61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "citeseer_dataset = Planetoid(root = \"Citeseer_dataset\", name = \"Citeseer\", transform = NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2746ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label names and colors\n",
    "label_dict = {\n",
    "    0: \"Agents\",\n",
    "    1: \"AI\",\n",
    "    2: \"DB\",\n",
    "    3: \"IR\",\n",
    "    4: \"ML\",\n",
    "    5: \"HCI\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf4de8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "6\n",
      "3703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(citeseer_dataset))\n",
    "print(citeseer_dataset.num_classes)\n",
    "print(citeseer_dataset.num_features)\n",
    "citeseer_graph = citeseer_dataset[0]\n",
    "citeseer_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f596aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citeseer_graph.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f95dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  120\n",
      "Validation samples:  500\n",
      "Test samples:  1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training samples: \", citeseer_graph.train_mask.sum().item())\n",
    "print(\"Validation samples: \", citeseer_graph.val_mask.sum().item())\n",
    "print(\"Test samples: \", citeseer_graph.test_mask.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68bebe8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 1, 5,  ..., 3, 1, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citeseer_graph.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "576ade26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 3327\n",
      "Number of edges: 9104\n",
      "Average node degree: 2.74\n",
      "Has isolated nodes: True\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of nodes: {citeseer_graph.num_nodes}')\n",
    "print(f'Number of edges: {citeseer_graph.num_edges}')\n",
    "print(f'Average node degree: {citeseer_graph.num_edges / citeseer_graph.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {citeseer_graph.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {citeseer_graph.has_self_loops()}')\n",
    "print(f'Is undirected: {citeseer_graph.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e6691ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_channels = input_channels\n",
    "\n",
    "        for hidden_units in hidden_channels:\n",
    "            layers.append(nn.Linear(in_features=in_channels, out_features=hidden_units))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_channels = hidden_units\n",
    "\n",
    "        layers.append(nn.Linear(in_features=in_channels, out_features=output_channels))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, data):\n",
    "         # only using node features (x)\n",
    "        x = data.x \n",
    "\n",
    "        output = self.layers(x)\n",
    "\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea937ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "citeseer_graph = citeseer_dataset[0].to(device)\n",
    "\n",
    "input_channels = citeseer_dataset.num_features\n",
    "\n",
    "hidden_channels = [16]\n",
    "#hidden_channels = [16, 32, 16]  # Three hidden layers with 16, 32, and 16 neurons respectively.\n",
    "\n",
    "output_channels = citeseer_dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08d9a7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3703, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP(\n",
    "    input_channels = input_channels, \n",
    "    hidden_channels = hidden_channels, \n",
    "    output_channels = output_channels).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "953fbc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  59366\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01, weight_decay = 5e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9288fad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010,                Train Loss: 1.714,                Train Acc: 0.825 Val Acc: 0.182\n",
      "Epoch: 020,                Train Loss: 1.530,                Train Acc: 1.000 Val Acc: 0.380\n",
      "Epoch: 030,                Train Loss: 1.265,                Train Acc: 1.000 Val Acc: 0.424\n",
      "Epoch: 040,                Train Loss: 0.964,                Train Acc: 1.000 Val Acc: 0.468\n",
      "Epoch: 050,                Train Loss: 0.708,                Train Acc: 1.000 Val Acc: 0.518\n",
      "Epoch: 060,                Train Loss: 0.533,                Train Acc: 1.000 Val Acc: 0.526\n",
      "Epoch: 070,                Train Loss: 0.424,                Train Acc: 1.000 Val Acc: 0.538\n",
      "Epoch: 080,                Train Loss: 0.355,                Train Acc: 1.000 Val Acc: 0.548\n",
      "Epoch: 090,                Train Loss: 0.308,                Train Acc: 1.000 Val Acc: 0.548\n",
      "Epoch: 100,                Train Loss: 0.274,                Train Acc: 1.000 Val Acc: 0.560\n",
      "Epoch: 110,                Train Loss: 0.249,                Train Acc: 1.000 Val Acc: 0.562\n",
      "Epoch: 120,                Train Loss: 0.229,                Train Acc: 1.000 Val Acc: 0.566\n",
      "Epoch: 130,                Train Loss: 0.211,                Train Acc: 1.000 Val Acc: 0.562\n",
      "Epoch: 140,                Train Loss: 0.194,                Train Acc: 1.000 Val Acc: 0.562\n",
      "Epoch: 150,                Train Loss: 0.180,                Train Acc: 1.000 Val Acc: 0.560\n",
      "Epoch: 160,                Train Loss: 0.166,                Train Acc: 1.000 Val Acc: 0.560\n",
      "Epoch: 170,                Train Loss: 0.154,                Train Acc: 1.000 Val Acc: 0.554\n",
      "Epoch: 180,                Train Loss: 0.144,                Train Acc: 1.000 Val Acc: 0.558\n",
      "Epoch: 190,                Train Loss: 0.135,                Train Acc: 1.000 Val Acc: 0.554\n",
      "Epoch: 200,                Train Loss: 0.128,                Train Acc: 1.000 Val Acc: 0.550\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(citeseer_graph)\n",
    "\n",
    "    loss = criterion(out[citeseer_graph.train_mask], citeseer_graph.y[citeseer_graph.train_mask])\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # Get predictions on the training data\n",
    "    pred_train = out.argmax(dim = 1)\n",
    "    \n",
    "    correct_train = (\n",
    "        pred_train[citeseer_graph.train_mask] == citeseer_graph.y[citeseer_graph.train_mask]\n",
    "    ).sum()\n",
    "    \n",
    "    acc_train = int(correct_train) / int(citeseer_graph.train_mask.sum())\n",
    "    \n",
    "    # Get predictions on validation data\n",
    "    model.eval()\n",
    "\n",
    "    pred_val = model(citeseer_graph).argmax(dim = 1)\n",
    "    \n",
    "    correct_val = (\n",
    "        pred_val[citeseer_graph.val_mask] == citeseer_graph.y[citeseer_graph.val_mask]\n",
    "    ).sum()\n",
    "    \n",
    "    acc_val = int(correct_val) / int(citeseer_graph.val_mask.sum())\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch: {epoch + 1:03d}, \\\n",
    "               Train Loss: {loss:.3f}, \\\n",
    "               Train Acc: {acc_train:.3f} Val Acc: {acc_val:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68ad48c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.595"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "pred = model(citeseer_graph).argmax(dim = 1)\n",
    "\n",
    "correct = (pred[citeseer_graph.test_mask] == citeseer_graph.y[citeseer_graph.test_mask]).sum()\n",
    "\n",
    "test_acc = int(correct) / int(citeseer_graph.test_mask.sum())\n",
    "\n",
    "test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
